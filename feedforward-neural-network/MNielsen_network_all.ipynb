{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/XiaoxueRan/hku_phys3151_2022/blob/main/feedforward-neural-network/MNielsen_network_all.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "dXoM7ZwcSdGv"
      },
      "outputs": [],
      "source": [
        "# A module to implement the gradient descent learning algorithm for a feedforward neural network.\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "class Network(object):\n",
        "    \n",
        "    # the list ''sizes'' contains the number of neurons in the respective layers of the network.\n",
        "    # [2, 3, 1] input layer 2 neurons, hidden layer 3 neurons, output layer 1 neuron\n",
        "    def __init__(self, sizes):\n",
        "\n",
        "        self.num_layers = len(sizes)\n",
        "        self.sizes = sizes\n",
        "        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
        "        self.weights = [np.random.randn(y, x)\n",
        "                        for x, y in zip(sizes[:-1], sizes[1:])]\n",
        "       \n",
        "    # return the output of the network if \"a\" is input.\n",
        "    def feedforward(self, a):\n",
        "        for b, w in zip(self.biases, self.weights):\n",
        "            a = sigmoid(np.dot(w, a)+b)\n",
        "        return a\n",
        "    \n",
        "    # “training_data” is a list of tuples \"(x,y)\" representing the training inputs and the desired outputs.\n",
        "    def SGD(self, training_data, epochs, mini_batch_size, eta, test_data=None):\n",
        "\n",
        "        training_data = list(training_data)\n",
        "        n = len(training_data)\n",
        "        \n",
        "        if test_data:\n",
        "            test_data = list(test_data)\n",
        "            n_test = len(test_data)\n",
        "            \n",
        "        for j in range(epochs):\n",
        "            random.shuffle(training_data)\n",
        "            mini_batches = [\n",
        "                training_data[k:k+mini_batch_size]\n",
        "                for k in range(0, n, mini_batch_size)]\n",
        "            for mini_batch in mini_batches:\n",
        "                self.update_mini_batch(mini_batch, eta)\n",
        "            if test_data:\n",
        "                print(\"Epoch {} : {} / {}\".format(j,self.evaluate(test_data),n_test));\n",
        "            else:\n",
        "                print(\"Epoch {} complete\".format(j))\n",
        "                \n",
        "    # update the network's weights and biases by applying gradient descent \n",
        "    # using backpropagation to a single mini batch.\n",
        "    # The ''mini_batch'' is a list of tuples \"(x,y)\", and \"eta\" is the learning rate.\n",
        "    def update_mini_batch(self, mini_batch, eta):\n",
        "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
        "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
        "        for x, y in mini_batch:\n",
        "            delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n",
        "            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
        "            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
        "        self.weights = [w-(eta/len(mini_batch))*nw\n",
        "                       for w, nw in zip(self.weights, nabla_w)]\n",
        "        self.biases = [b-(eta/len(mini_batch))*nb\n",
        "                       for b, nb in zip(self.biases, nabla_b)]\n",
        "        \n",
        "    # return a tuple \"(nabla_b, nabla_w)\" representing the gradient for the cost function.\n",
        "    # \"nabla_b\" and \"nabla_w\" are layer-by-layer lists of numpy arrays.\n",
        "    def backprop(self, x, y):\n",
        "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
        "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
        "        # feedforward\n",
        "        activation = x\n",
        "        activations = [x] # list to store all the activations, layer by layer\n",
        "        zs = [] # list to store all the z vectors, layer by layer\n",
        "        for b, w in zip(self.biases, self.weights):\n",
        "            z = np.dot(w, activation) + b\n",
        "            zs.append(z)\n",
        "            activation = sigmoid(z)\n",
        "            activations.append(activation)\n",
        "        # backward pass\n",
        "        delta = self.cost_derivative(activations[-1], y) #* \\\n",
        "  #          sigmoid_prime(zs[-1])\n",
        "        nabla_b[-1] = delta\n",
        "        nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
        "        # l=1 means the last layer of the neurons\n",
        "        # l=2 means the second-last layer\n",
        "        for l in range(2, self.num_layers):\n",
        "            z = zs[-l]\n",
        "            sp = sigmoid_prime(z)\n",
        "            delta = np.dot(self.weights[-l+1].transpose(), delta) * sp\n",
        "            nabla_b[-l] = delta\n",
        "            nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
        "        return (nabla_b, nabla_w)\n",
        "    \n",
        "    # the neural network's output is assumed to be the index of \n",
        "    # whichever neuron in the final layer has the highest activation.\n",
        "    def evaluate(self, test_data):\n",
        "        test_results = [(np.argmax(self.feedforward(x)),y)\n",
        "                       for (x,y) in test_data]\n",
        "        return sum(int(x == y) for (x,y) in test_results)\n",
        "    \n",
        "    # vector of partial derivatives \\partial C_X / \\partial a for the output activations.\n",
        "    def cost_derivative(self, output_activations, y):\n",
        "        return (output_activations-y)\n",
        "    \n",
        "# Miscellaneous functions\n",
        "def sigmoid(z):\n",
        "    return 1.0/(1.0+np.exp(-z))\n",
        "    \n",
        "def sigmoid_prime(z):\n",
        "    return sigmoid(z)*(1-sigmoid(z))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/XiaoxueRan/hku_phys3151_2022"
      ],
      "metadata": {
        "id": "RoEQB1tkTO4i",
        "outputId": "1f9d2fff-7f0c-4dd2-ef1d-127332291ee6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'hku_phys3151_2022'...\n",
            "remote: Enumerating objects: 294, done.\u001b[K\n",
            "remote: Counting objects: 100% (294/294), done.\u001b[K\n",
            "remote: Compressing objects: 100% (275/275), done.\u001b[K\n",
            "remote: Total 294 (delta 154), reused 79 (delta 19), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (294/294), 25.69 MiB | 15.74 MiB/s, done.\n",
            "Resolving deltas: 100% (154/154), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "9bt3SXZUSdGy"
      },
      "outputs": [],
      "source": [
        "# A library to load the MNIST image data.\n",
        "import pickle\n",
        "import gzip\n",
        "import matplotlib.cm as cm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Return the MNIST data as a tuple containing the training data, the validation data, and the test data.\n",
        "# The \"training_data\" is returned as a tuple with two entries.\n",
        "\n",
        "# The first entry contains the actual training images. This is a numpy ndarray with 50,000 entries. Each entry is, \n",
        "# in turn, a numpy ndarray with 784 values, representing the 28 * 28 = 784 pixels in a single MNIST image.\n",
        "\n",
        "# The second entry in the \"training_data\" tuple is a numpy ndarray containing 50,000 entries. Those entries\n",
        "# are just the digit values (0,1,...,9) for the corresponding images contained in the first entry of the tuple.\n",
        "\n",
        "# The \"validation_data\" and \"test_data\" are similar, except each contains only 10,000 images.\n",
        "def load_data():\n",
        "\n",
        "        f = gzip.open('/content/hku_phys3151_2022/feedforward-neural-network/mnist.pkl.gz','rb')\n",
        "        training_data, validation_data, test_data = pickle.load(f, encoding=\"latin1\")\n",
        "        f.close()\n",
        "        return (training_data, validation_data, test_data)\n",
        "\n",
        "# Return a tuple containing \"(training_data, validation_data, test_data)\".\n",
        "# \"training_data\" is a list containing 50,000 2-tuples \"(x,y)\". \n",
        "# \"x\" is a 784-dimensional numpy.ndarray containing the input image.\n",
        "# \"y\" is a 10-dimensional numpy.ndarray representing the unit vector \n",
        "# corresponding to the correct digit for \"x\".\n",
        "# \"validation_data\" and \"test_data\" are lists containing 10,000 2-tuples \"(x,y)\". In each case,\n",
        "# \"x\" is a 784-dimensional numpy.ndarray containing the input image, and\n",
        "# \"y\" is the corresponding classification, i.e., the digit values (integers) corresponding to \"x\".\n",
        "def load_data_wrapper():\n",
        "    tr_d, va_d, te_d = load_data()\n",
        "    training_inputs = [np.reshape(x, (784,1)) for x in tr_d[0]]\n",
        "    \n",
        "    plt.imshow(training_inputs[9].reshape((28,28)),cmap=cm.Greys_r)\n",
        "    plt.show()\n",
        "    \n",
        "    training_results = [vectorized_result(y) for y in tr_d[1]]\n",
        "    training_data = zip(training_inputs, training_results)\n",
        "    validation_inputs = [np.reshape(x, (784,1)) for x in va_d[0]]\n",
        "    validation_data = zip(validation_inputs, va_d[1])\n",
        "    test_inputs = [np.reshape(x, (784,1)) for x in te_d[0]]\n",
        "    test_data = zip(test_inputs, te_d[1])\n",
        "    return (training_data, validation_data, test_data)\n",
        "\n",
        "# Return a 10-dimensional unit vector with a 1.0 in the jth position and zeros elsewhere.\n",
        "# This is used to convert a digit (0...9) into a corresponding desired output from the neural network.\n",
        "def vectorized_result(j):\n",
        "    e = np.zeros((10,1))\n",
        "    e[j] = 1.0\n",
        "    return e    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "5k-jTJDrSdGy",
        "outputId": "71ba5c4f-4224-46d7-fdd9-8874aa3bf254",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAANkElEQVR4nO3dXaxVdXrH8d+v6hgVY6BYRCCVoiaOTepUoiaaZohxtF5wnBgnw0UjqcokQmSSXlSnMWNSG1E79MKLiWeESHWKju8ETUdLsNibUTRWUGR8ARzIAYJcjC/RqfD04iwmRzz7vw97r73X5jzfT3Ky917PXms9bv251t7r5e+IEIDJ70+abgBAfxB2IAnCDiRB2IEkCDuQxIn9XJltfvoHeiwiPN70rrbstq+xvd32+7Zv72ZZAHrLnR5nt32CpN9KukrSbkmvSVoUEe8U5mHLDvRYL7bsl0h6PyI+jIg/SHpM0lAXywPQQ92EfZak3415vbua9jW2l9jebHtzF+sC0KWe/0AXEcOShiV244EmdbNl3yNpzpjXs6tpAAZQN2F/TdJ5tufa/pakH0paV09bAOrW8W58RHxle5mkX0s6QdLqiHi7ts4A1KrjQ28drYzv7EDP9eSkGgDHD8IOJEHYgSQIO5AEYQeSIOxAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkujrkM3A8WLbtm1dzX/BBRfU1El92LIDSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBIcZ0dKjz32WLF+7rnnFusvvvhine30RVdht71T0ieSDkn6KiLm19EUgPrVsWVfEBEHalgOgB7iOzuQRLdhD0kv2n7d9pLx3mB7ie3Ntjd3uS4AXeh2N/6KiNhj+88kvWT73YjYNPYNETEsaViSbEeX6wPQoa627BGxp3rcL+kZSZfU0RSA+nUcdtun2T79yHNJ35O0ta7GANSrm934GZKesX1kOf8REf9ZS1dADR5++OGWteuvv74476FDh4r19evXd9JSozoOe0R8KOmvauwFQA9x6A1IgrADSRB2IAnCDiRB2IEkHNG/k9o4gw79tH379pa1dpewvvvuu8X6hRde2FFP/RARHm86W3YgCcIOJEHYgSQIO5AEYQeSIOxAEoQdSIJbSU9yQ0NDxfrdd99drC9YsKBYP3CguXuNLlu2rFifM2dOy9rBgweL895yyy0d9TTI2LIDSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBJczz7J7d+/v1ifPn16sb5w4cJivclbKo+MjBTrZ511VsvazTffXJx31apVHfU0CLieHUiOsANJEHYgCcIOJEHYgSQIO5AEYQeS4Hr2Se7LL78s1tudZ3HqqafW2c4xufzyy4v1qVOnFuulf7ZTTjmlo56OZ2237LZX295ve+uYadNsv2T7veqx/KkDaNxEduMflnTNUdNul7QhIs6TtKF6DWCAtQ17RGySdPQ9fIYkramer5F0Xc19AahZp9/ZZ0TEkROT90qa0eqNtpdIWtLhegDUpOsf6CIiShe4RMSwpGGJC2GAJnV66G2f7ZmSVD2WL60C0LhOw75O0o3V8xslPVdPOwB6pe1uvO21kr4rabrt3ZJ+KmmFpF/ZvknSLkk/6GWTKBseHm5ZO/vss4vztrvefdOmTR31NBFTpkwp1lesWFGsn3TSScX6jh07WtYefPDB4ryTUduwR8SiFqUra+4FQA9xuiyQBGEHkiDsQBKEHUiCsANJcCvp48DcuXOL9S1btrSsnXzyycV5b7jhhmL92WefLda78fzzzxfr11xz9PVXX/fpp58W62ecccYx9zQZcCtpIDnCDiRB2IEkCDuQBGEHkiDsQBKEHUiCW0kPgEsvvbRYf+GFF4r10m2Rn3jiieK8vTyOLkn33ntvy9rVV1/d1bLvu+++rubPhi07kARhB5Ig7EAShB1IgrADSRB2IAnCDiTB9ew1OPHE8ukKy5cvL9bvv//+Yt0e9/LkPyr9O9y1a1dx3nbH2e+4445i/cwzzyzWX3nllZa12bNnF+fduHFjsX7VVVcV61lxPTuQHGEHkiDsQBKEHUiCsANJEHYgCcIOJMFx9hq0O46+cuXKrpbf7jj7xx9/3LI2bdq0rtb90UcfFevtll8alvnzzz8vznv66acX6xhfx8fZba+2vd/21jHT7rK9x/ab1d+1dTYLoH4T2Y1/WNJ4Q3P8W0RcVP2Vb6UCoHFtwx4RmyQd7EMvAHqomx/oltl+q9rNn9rqTbaX2N5se3MX6wLQpU7D/nNJ8yRdJGlE0s9avTEihiNifkTM73BdAGrQUdgjYl9EHIqIw5J+IemSetsCULeOwm575piX35e0tdV7AQyGtsfZba+V9F1J0yXtk/TT6vVFkkLSTkk/ioiRtis7jo+z33rrrS1rDzzwQHHew4cPF+tffPFFsb548eJife/evS1rDz30UHHe888/v1hvp5tr7dtpN/76ZZddVqxv27at43Ufz1odZ287SERELBpn8qquOwLQV5wuCyRB2IEkCDuQBGEHkiDsQBJc4jpBpcM4M2bMKM57zz33FOvtbiXdjYsvvrhYf/zxx4v1uXPnFuu9PPS2adOmYn3BggUdL3sy41bSQHKEHUiCsANJEHYgCcIOJEHYgSQIO5BE26veMOrJJ59sWVu9enVx3h07dtTdzoS1GxZ51qxZXS1/6dKlxfqrr77a8bI/+OCDjufFN7FlB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkuJ59Epg6teXoW23PARgaGirWDx4sD/M3ffr0Yh39x/XsQHKEHUiCsANJEHYgCcIOJEHYgSQIO5AE17NPAnfeeWfL2sKFC4vzfvbZZ8X6/PnzO+oJg6ftlt32HNsbbb9j+23by6vp02y/ZPu96rH1mR0AGjeR3fivJP1DRHxb0mWSltr+tqTbJW2IiPMkbaheAxhQbcMeESMR8Ub1/BNJ2yTNkjQkaU31tjWSrutVkwC6d0zf2W2fI+k7kn4jaUZEjFSlvZLGHfDM9hJJSzpvEUAdJvxrvO0pkp6S9OOI+P3YWoxeTTPuRS4RMRwR8yOCX3qABk0o7LZP0mjQfxkRT1eT99meWdVnStrfmxYB1KHtbrxHx+RdJWlbRKwcU1on6UZJK6rH53rSITRv3rxiffHixR0v+9FHHy3Wd+7c2fGyMVgm8p39ckl/J2mL7TeraT/RaMh/ZfsmSbsk/aA3LQKoQ9uwR8T/SBr3YnhJV9bbDoBe4XRZIAnCDiRB2IEkCDuQBGEHkuBW0seBAwcOFOulW0m//PLLxXmvvJIDKpMNt5IGkiPsQBKEHUiCsANJEHYgCcIOJEHYgSS4lfRx4JFHHinWb7vttpa1tWvX1t0OjlNs2YEkCDuQBGEHkiDsQBKEHUiCsANJEHYgCa5nByYZrmcHkiPsQBKEHUiCsANJEHYgCcIOJEHYgSTaht32HNsbbb9j+23by6vpd9neY/vN6u/a3rcLoFNtT6qxPVPSzIh4w/bpkl6XdJ1Gx2P/NCL+dcIr46QaoOdanVQzkfHZRySNVM8/sb1N0qx62wPQa8f0nd32OZK+I+k31aRltt+yvdr2uGMQ2V5ie7PtzV11CqArEz433vYUSf8t6V8i4mnbMyQdkBSS/lmju/p/32YZ7MYDPdZqN35CYbd9kqT1kn4dESvHqZ8jaX1E/GWb5RB2oMc6vhDGtiWtkrRtbNCrH+6O+L6krd02CaB3JvJr/BWSXpG0RdLhavJPJC2SdJFGd+N3SvpR9WNeaVls2YEe62o3vi6EHeg9rmcHkiPsQBKEHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeSIOxAEoQdSIKwA0kQdiAJwg4k0faGkzU7IGnXmNfTq2mDaFB7G9S+JHrrVJ29/XmrQl+vZ//Gyu3NETG/sQYKBrW3Qe1LordO9as3duOBJAg7kETTYR9ueP0lg9rboPYl0Vun+tJbo9/ZAfRP01t2AH1C2IEkGgm77Wtsb7f9vu3bm+ihFds7bW+phqFudHy6agy9/ba3jpk2zfZLtt+rHscdY6+h3gZiGO/CMOONfnZND3/e9+/stk+Q9FtJV0naLek1SYsi4p2+NtKC7Z2S5kdE4ydg2P4bSZ9K+vcjQ2vZvk/SwYhYUf2PcmpE/OOA9HaXjnEY7x711mqY8cVq8LOrc/jzTjSxZb9E0vsR8WFE/EHSY5KGGuhj4EXEJkkHj5o8JGlN9XyNRv9j6bsWvQ2EiBiJiDeq559IOjLMeKOfXaGvvmgi7LMk/W7M690arPHeQ9KLtl+3vaTpZsYxY8wwW3slzWiymXG0Hca7n44aZnxgPrtOhj/vFj/QfdMVEfHXkv5W0tJqd3Ugxeh3sEE6dvpzSfM0OgbgiKSfNdlMNcz4U5J+HBG/H1tr8rMbp6++fG5NhH2PpDljXs+upg2EiNhTPe6X9IxGv3YMkn1HRtCtHvc33M8fRcS+iDgUEYcl/UINfnbVMONPSfplRDxdTW78sxuvr359bk2E/TVJ59mea/tbkn4oaV0DfXyD7dOqH05k+zRJ39PgDUW9TtKN1fMbJT3XYC9fMyjDeLcaZlwNf3aND38eEX3/k3StRn+R/0DSPzXRQ4u+/kLS/1Z/bzfdm6S1Gt2t+z+N/rZxk6Q/lbRB0nuS/kvStAHq7RGNDu39lkaDNbOh3q7Q6C76W5LerP6ubfqzK/TVl8+N02WBJPiBDkiCsANJEHYgCcIOJEHYgSQIO5AEYQeS+H+kHlOXTkCpbwAAAABJRU5ErkJggg==\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "training_data, validation_data, test_data = load_data_wrapper()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "iqQPTo_mSdGz"
      },
      "outputs": [],
      "source": [
        "net = Network([784,100,10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "k2frayMZSdGz",
        "outputId": "922c247d-b369-47e0-b5a5-72deeb6a018c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 : 2548 / 10000\n",
            "Epoch 1 : 5499 / 10000\n",
            "Epoch 2 : 6540 / 10000\n",
            "Epoch 3 : 7050 / 10000\n",
            "Epoch 4 : 7388 / 10000\n",
            "Epoch 5 : 7649 / 10000\n",
            "Epoch 6 : 7880 / 10000\n",
            "Epoch 7 : 7998 / 10000\n",
            "Epoch 8 : 8118 / 10000\n",
            "Epoch 9 : 8227 / 10000\n",
            "Epoch 10 : 8302 / 10000\n",
            "Epoch 11 : 8358 / 10000\n",
            "Epoch 12 : 8423 / 10000\n",
            "Epoch 13 : 8493 / 10000\n",
            "Epoch 14 : 8538 / 10000\n",
            "Epoch 15 : 8584 / 10000\n",
            "Epoch 16 : 8625 / 10000\n",
            "Epoch 17 : 8664 / 10000\n",
            "Epoch 18 : 8685 / 10000\n",
            "Epoch 19 : 8712 / 10000\n",
            "Epoch 20 : 8741 / 10000\n",
            "Epoch 21 : 8765 / 10000\n",
            "Epoch 22 : 8784 / 10000\n",
            "Epoch 23 : 8806 / 10000\n",
            "Epoch 24 : 8828 / 10000\n",
            "Epoch 25 : 8847 / 10000\n",
            "Epoch 26 : 8862 / 10000\n",
            "Epoch 27 : 8886 / 10000\n",
            "Epoch 28 : 8905 / 10000\n",
            "Epoch 29 : 8920 / 10000\n"
          ]
        }
      ],
      "source": [
        "net.SGD(training_data, 30, 10000, 3.0, test_data=test_data)     "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "LRs0ll3ISdGz"
      },
      "outputs": [],
      "source": [
        "# An improved version of MNielsen_network, implementing the SGD learning algorithm for a feedforward neural network.\n",
        "# Improvments include \n",
        "# 1. the cross-entropy cost function, \n",
        "# 2. regularization, \n",
        "# 3. better initialization of the weights.\n",
        "\n",
        "import json\n",
        "import random\n",
        "import sys\n",
        "import numpy as np\n",
        "\n",
        "# define the quadratic and cross-entropy cost functions\n",
        "\n",
        "class QuadraticCost(object):\n",
        "    \n",
        "    # return the cost associated with an output \"a\" and desired output \"y\".\n",
        "    @staticmethod\n",
        "    def fn(a, y):\n",
        "        return 0.5*np.linalg.norm(a-y)**2\n",
        "    \n",
        "    # return the error delta from the output layer.\n",
        "    @staticmethod\n",
        "    def delta(z, a, y):\n",
        "        return (a-y)*sigmoid_prime(z)\n",
        "    \n",
        "class CrossEntropyCost(object):\n",
        "    \n",
        "    # return the cost associated with an output \"a\" and desired output \"y\".\n",
        "    # np.nan_to_num is used to ensure numerical stability, make sure 0*log(0) = 0.0\n",
        "    @staticmethod\n",
        "    def fn(a, y):\n",
        "        return np.sum(np.nan_to_num(-y*np.log(a)-(1-y)*np.log(1-a)))\n",
        "    \n",
        "    # returm the error delta from the output layer.\n",
        "    # parameter \"z\" is not used by the method. \n",
        "    @staticmethod\n",
        "    def delta(z, a, y):\n",
        "        return (a-y)\n",
        "    \n",
        "# Main Network class\n",
        "class Network2(object):\n",
        "    \n",
        "    \n",
        "    def __init__(self, sizes, cost=CrossEntropyCost):\n",
        "        \n",
        "        # the biases and weights for the network are initiated randomly, using \"self.default_weight_initializer\".\n",
        "        self.num_layers = len(sizes)\n",
        "        self.sizes = sizes\n",
        "        self.default_weight_initializer()\n",
        "        self.cost = cost\n",
        "    \n",
        "    def default_weight_initializer(self):\n",
        "        \n",
        "        # initialize each weight using a Gaussian distribution with mean 0 and standard deviation 1\n",
        "        # over the square root of the number of weights connecting to the same neuron. \n",
        "        # initialize the biases using a Gaussian distribution with mean 0 and standard deviation 1.\n",
        "    \n",
        "        # for the input layers, there is no biases.\n",
        "        self.biases = [np.random.randn(y,1) for y in self.sizes[1:]]\n",
        "        self.weights = [np.random.randn(y, x)/np.sqrt(x) for x, y in zip(self.sizes[:-1], self.sizes[1:])]\n",
        "\n",
        "    def large_weight_initializer(self):\n",
        "            \n",
        "        self.biases = [np.random.randn(y,1) for y in self.sizes[1:]]\n",
        "        self.weights = [np.random.randn(y, x) for x, y in zip(self.sizes[:-1], self.sizes[1:])]\n",
        "        \n",
        "    def feedforward(self, a):\n",
        "        for b, w in zip(self.biases, self.weights):\n",
        "            a = sigmoid(np.dot(w, a) + b)\n",
        "        return a\n",
        "    \n",
        "    # \"evaluation_data\", monitor the cost and accuracy on either the evaluation data or the training data.\n",
        "    # returns a tuple containing four lists:\n",
        "    # 1. the (per-epoch) costs on the evaluation data,\n",
        "    # 2. the accuracies on the evaluation data,\n",
        "    # 3. the costs on the training data,\n",
        "    # 4. the accuracies on the training data.\n",
        "    # If we train for 30 epochs, then the first element of the tuple will be a \n",
        "    # 30-element list containing the cost on the evaluation data at the end of each epoch.\n",
        "    def SGD(self, training_data, epochs, mini_batch_size, eta,\n",
        "            lmbda=0.0,\n",
        "            evaluation_data = None,\n",
        "            monitor_evaluation_cost = False,\n",
        "            monitor_evaluation_accuracy = False,\n",
        "            monitor_training_cost = False,\n",
        "            monitor_training_accuracy = False,\n",
        "            early_stopping_n = 0):\n",
        "        \n",
        "        # early stopping functionality:\n",
        "        best_accuracy=1\n",
        "        \n",
        "        training_data = list(training_data)\n",
        "        n = len(training_data)\n",
        "        \n",
        "        if evaluation_data:\n",
        "            evaluation_data = list(evaluation_data)\n",
        "            n_data = len(evaluation_data)\n",
        "            \n",
        "        # early stopping functionality:\n",
        "        best_accuracy=0\n",
        "        no_accuracy_change=0\n",
        "        \n",
        "        evaluation_cost, evaluation_accuracy = [], []\n",
        "        training_cost, training_accuracy = [], []\n",
        "        for j in range(epochs):\n",
        "            random.shuffle(training_data)\n",
        "            mini_batches = [training_data[k:k+mini_batch_size] for k in range(0, n, mini_batch_size)]\n",
        "            for mini_batch in mini_batches:\n",
        "                self.update_mini_batch(mini_batch, eta, lmbda, len(training_data))\n",
        "                \n",
        "            print(\"Epoch %s training complete\" % j)\n",
        "            \n",
        "            if monitor_training_cost:\n",
        "                cost = self.total_cost(training_data, lmbda)\n",
        "                training_cost.append(cost)\n",
        "                print(\"Cost on training data: {}\".format(cost))\n",
        "            if monitor_training_accuracy:\n",
        "                accuracy = self.accuracy(training_data, convert=True)\n",
        "                training_accuracy.append(accuracy)\n",
        "                print(\"Accuracy on training data: {} / {}\".format(accuracy, n))\n",
        "            if monitor_evaluation_cost:\n",
        "                cost = self.total_cost(evaluation_data, lmbda, convert=True)\n",
        "                evaluation_cost.append(cost)\n",
        "                print(\"Cost on evaluation data: {}\".format(cost))\n",
        "            if monitor_evaluation_accuracy:\n",
        "                accuracy = self.accuracy(evaluation_data)\n",
        "                evaluation_accuracy.append(accuracy)\n",
        "                print(\"Accuracy on evaluation data: {} / {}\".format(self.accuracy(evaluation_data), n_data))\n",
        "            \n",
        "            print()\n",
        "            \n",
        "            # Early stopping:\n",
        "            if early_stopping_n > 0:\n",
        "                if accuracy > best_accuracy:\n",
        "                    best_accuracy = accuracy\n",
        "                    no_accuracy_change = 0\n",
        "                    print(\"Early-stopping: Best so far{}\".format(best_accuracy))\n",
        "                else:\n",
        "                    no_accuracy_change += 1\n",
        "                    \n",
        "                if (no_accuracy_change == early_stopping_n):\n",
        "                    print(\"Early-stopping: No accuracy cahnge in last epochs: {}\".format(early_stopping_n))\n",
        "                    return evaluation_cost, evaluation_accuracy, training_cost, training_accuracy\n",
        "        \n",
        "        return evaluation_cost, evaluation_accuracy, training_cost, training_accuracy\n",
        "    \n",
        "    \n",
        "    # \"mini_batch\" is a list of tuples \"(x,y)\"\n",
        "    # \"eta\" is the learning rate\n",
        "    # \"lmbda\" is the regularization parameter\n",
        "    # \"n\" is the total size of the training set.\n",
        "    def update_mini_batch(self, mini_batch, eta, lmbda, n):\n",
        "        \n",
        "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
        "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
        "        for x, y in mini_batch:\n",
        "            delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n",
        "            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
        "            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
        "        # \"eta*(lmbda/n)*w\" comes from the regularization term.\n",
        "        self.weights = [(1-eta*(lmbda/n))*w-(eta/len(mini_batch))*nw for w, nw in zip(self.weights, nabla_w)]\n",
        "        self.biases = [b-(eta/len(mini_batch))*nb for b, nb in zip(self.biases, nabla_b)]\n",
        "        \n",
        "    # return a tuple \"(nabla_b, nabla_w)\" representing the gradient for the cost function.\n",
        "    # \"nabla_b\" and \"nabla_w\" are layer-by-layer lists of numpy arrays.\n",
        "    def backprop(self, x, y):\n",
        "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
        "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
        "        \n",
        "        # feedforward\n",
        "        activation = x\n",
        "        activations = [x] # list to store all the activations, layer by layer\n",
        "        zs = [] # list to store all the z vectors, layer by layer\n",
        "        for b, w in zip(self.biases, self.weights):\n",
        "            z = np.dot(w, activation) + b\n",
        "            zs.append(z)\n",
        "            activation = sigmoid(z)\n",
        "            activations.append(activation)\n",
        "            \n",
        "        # backpropagate\n",
        "        delta = (self.cost).delta(zs[-1], activations[-1], y)\n",
        "        nabla_b[-1] = delta\n",
        "        nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
        "        \n",
        "        for l in range(2, self.num_layers):\n",
        "            z = zs[-l]\n",
        "            sp = sigmoid_prime(z)\n",
        "            delta = np.dot(self.weights[-l+1].transpose(), delta) * sp\n",
        "            nabla_b[-l] = delta\n",
        "            nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
        "        return (nabla_b, nabla_w)\n",
        "    \n",
        "    \n",
        "    def accuracy(self, data, convert=False):\n",
        "     \n",
        "        if convert:\n",
        "            results = [(np.argmax(self.feedforward(x)), np.argmax(y)) for (x, y) in data]\n",
        "        else:\n",
        "            results = [(np.argmax(self.feedforward(x)), y) for (x, y) in data]\n",
        "        \n",
        "        result_accuracy = sum(int(x==y) for (x,y) in results)\n",
        "        return result_accuracy\n",
        "    \n",
        "    def total_cost(self, data, lmbda, convert=False):\n",
        "        cost = 0.0\n",
        "        for x, y in data:\n",
        "            a = self.feedforward(x)\n",
        "            if convert: y = vectorized_result(y)\n",
        "            cost += self.cost.fn(a, y)/len(data)\n",
        "            cost += 0.5*(lmbda/len(data))*sum(np.linalg.norm(w)**2 for w in self.weights)\n",
        "        return cost\n",
        "    \n",
        "    # Save the neural network to the file \"filename\".\n",
        "    def save(self, filename):\n",
        "        data = {\"sizes\": self.sizes,\n",
        "                \"weights\": [w.tolist() for w in self.weights],\n",
        "                \"biases\": [b.tolist() for b in self.biases],\n",
        "                \"cost\": str(self.cost.__name__)}\n",
        "        f = open(filename, \"w\")\n",
        "        json.dump(data, f)\n",
        "        f.close()\n",
        "        \n",
        "        \n",
        "# Loading a Network\n",
        "def load(filename):\n",
        "    \n",
        "    f = open(filename, \"r\")\n",
        "    data = json.load(f)\n",
        "    f.close()\n",
        "    cost = getattr(sys.modules[__name__], data[\"cost\"])\n",
        "    net = Network(data[\"sizes\"], cost=cost)\n",
        "    net.weights = [np.array(w) for w in data[\"weights\"]]\n",
        "    net.biases = [np.array(b) for b in data[\"biases\"]]\n",
        "    return net\n",
        "\n",
        "# Miscellaneous functions\n",
        "def vectorized_result(j):\n",
        "    \n",
        "    e = np.zeros((10,1))\n",
        "    e[j] = 1.0\n",
        "    return e\n",
        "\n",
        "def sigmoid(z):\n",
        "    return 1.0/(1.0+np.exp(-z))\n",
        "\n",
        "def sigmoid_prime(z):\n",
        "    return sigmoid(z)*(1-sigmoid(z))\n",
        "        \n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# A library to load the MNIST image data.\n",
        "import pickle\n",
        "import gzip\n",
        "import matplotlib.cm as cm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Return the MNIST data as a tuple containing the training data, the validation data, and the test data.\n",
        "# The \"training_data\" is returned as a tuple with two entries.\n",
        "\n",
        "# The first entry contains the actual training images. This is a numpy ndarray with 50,000 entries. Each entry is, \n",
        "# in turn, a numpy ndarray with 784 values, representing the 28 * 28 = 784 pixels in a single MNIST image.\n",
        "\n",
        "# The second entry in the \"training_data\" tuple is a numpy ndarray containing 50,000 entries. Those entries\n",
        "# are just the digit values (0,1,...,9) for the corresponding images contained in the first entry of the tuple.\n",
        "\n",
        "# The \"validation_data\" and \"test_data\" are similar, except each contains only 10,000 images.\n",
        "def load_data():\n",
        "\n",
        "        f = gzip.open('/content/hku_phys3151_2022/feedforward-neural-network/mnist.pkl.gz','rb')\n",
        "        training_data, validation_data, test_data = pickle.load(f, encoding=\"latin1\")\n",
        "        f.close()\n",
        "        return (training_data, validation_data, test_data)\n",
        "\n",
        "# Return a tuple containing \"(training_data, validation_data, test_data)\".\n",
        "# \"training_data\" is a list containing 50,000 2-tuples \"(x,y)\". \n",
        "# \"x\" is a 784-dimensional numpy.ndarray containing the input image.\n",
        "# \"y\" is a 10-dimensional numpy.ndarray representing the unit vector \n",
        "# corresponding to the correct digit for \"x\".\n",
        "# \"validation_data\" and \"test_data\" are lists containing 10,000 2-tuples \"(x,y)\". In each case,\n",
        "# \"x\" is a 784-dimensional numpy.ndarray containing the input image, and\n",
        "# \"y\" is the corresponding classification, i.e., the digit values (integers) corresponding to \"x\".\n",
        "def load_data_wrapper():\n",
        "    tr_d, va_d, te_d = load_data()\n",
        "    training_inputs = [np.reshape(x, (784,1)) for x in tr_d[0]]\n",
        "    \n",
        "    plt.imshow(training_inputs[1].reshape((28,28)),cmap=cm.Greys_r)\n",
        "    plt.show()\n",
        "    \n",
        "    training_results = [vectorized_result(y) for y in tr_d[1]]\n",
        "    training_data = zip(training_inputs, training_results)\n",
        "    validation_inputs = [np.reshape(x, (784,1)) for x in va_d[0]]\n",
        "    validation_data = zip(validation_inputs, va_d[1])\n",
        "    test_inputs = [np.reshape(x, (784,1)) for x in te_d[0]]\n",
        "    test_data = zip(test_inputs, te_d[1])\n",
        "    return (training_data, validation_data, test_data)"
      ],
      "metadata": {
        "id": "4KCSryRc-XA5"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_data, validation_data, test_data = load_data_wrapper()"
      ],
      "metadata": {
        "id": "czAce4vR-Y_-",
        "outputId": "5e8aa519-04ca-418e-a95a-8db49846a620",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "net2 = Network2([784, 30, 10], cost=CrossEntropyCost)\n",
        "net2.SGD(training_data, 30, 10, 0.5,lmbda = 5.0,evaluation_data=validation_data,monitor_evaluation_accuracy=True,\n",
        "        monitor_evaluation_cost=True,monitor_training_accuracy=True,monitor_training_cost=True)"
      ],
      "metadata": {
        "id": "m3vhgcz_-bEl",
        "outputId": "50958846-dc77-494f-b5ec-319fdc418f62",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 training complete\n",
            "Cost on training data: 3873.6261422439547\n",
            "Accuracy on training data: 47021 / 50000\n",
            "Cost on evaluation data: 3873.626380047432\n",
            "Accuracy on evaluation data: 9407 / 10000\n",
            "\n",
            "Epoch 1 training complete\n",
            "Cost on training data: 4903.894639603323\n",
            "Accuracy on training data: 47467 / 50000\n",
            "Cost on evaluation data: 4903.909219502273\n",
            "Accuracy on evaluation data: 9488 / 10000\n",
            "\n",
            "Epoch 2 training complete\n",
            "Cost on training data: 5470.352651205806\n",
            "Accuracy on training data: 47731 / 50000\n",
            "Cost on evaluation data: 5470.3780924437515\n",
            "Accuracy on evaluation data: 9507 / 10000\n",
            "\n",
            "Epoch 3 training complete\n",
            "Cost on training data: 5749.527335650388\n",
            "Accuracy on training data: 48089 / 50000\n",
            "Cost on evaluation data: 5749.556195050321\n",
            "Accuracy on evaluation data: 9571 / 10000\n",
            "\n",
            "Epoch 4 training complete\n",
            "Cost on training data: 6006.232150178873\n",
            "Accuracy on training data: 47925 / 50000\n",
            "Cost on evaluation data: 6006.274159741172\n",
            "Accuracy on evaluation data: 9511 / 10000\n",
            "\n",
            "Epoch 5 training complete\n",
            "Cost on training data: 6127.03040749656\n",
            "Accuracy on training data: 48070 / 50000\n",
            "Cost on evaluation data: 6127.0597776886025\n",
            "Accuracy on evaluation data: 9563 / 10000\n",
            "\n",
            "Epoch 6 training complete\n",
            "Cost on training data: 6234.335571196147\n",
            "Accuracy on training data: 48138 / 50000\n",
            "Cost on evaluation data: 6234.38211034149\n",
            "Accuracy on evaluation data: 9570 / 10000\n",
            "\n",
            "Epoch 7 training complete\n",
            "Cost on training data: 6304.60879505393\n",
            "Accuracy on training data: 48039 / 50000\n",
            "Cost on evaluation data: 6304.652229312478\n",
            "Accuracy on evaluation data: 9536 / 10000\n",
            "\n",
            "Epoch 8 training complete\n",
            "Cost on training data: 6354.417743889218\n",
            "Accuracy on training data: 47809 / 50000\n",
            "Cost on evaluation data: 6354.4519082848\n",
            "Accuracy on evaluation data: 9503 / 10000\n",
            "\n",
            "Epoch 9 training complete\n",
            "Cost on training data: 6384.07506466922\n",
            "Accuracy on training data: 48150 / 50000\n",
            "Cost on evaluation data: 6384.110704739801\n",
            "Accuracy on evaluation data: 9556 / 10000\n",
            "\n",
            "Epoch 10 training complete\n",
            "Cost on training data: 6454.477543697938\n",
            "Accuracy on training data: 48202 / 50000\n",
            "Cost on evaluation data: 6454.511532804825\n",
            "Accuracy on evaluation data: 9563 / 10000\n",
            "\n",
            "Epoch 11 training complete\n",
            "Cost on training data: 6475.327314437201\n",
            "Accuracy on training data: 48104 / 50000\n",
            "Cost on evaluation data: 6475.362266120982\n",
            "Accuracy on evaluation data: 9556 / 10000\n",
            "\n",
            "Epoch 12 training complete\n",
            "Cost on training data: 6538.964339594958\n",
            "Accuracy on training data: 48187 / 50000\n",
            "Cost on evaluation data: 6539.0050191049995\n",
            "Accuracy on evaluation data: 9559 / 10000\n",
            "\n",
            "Epoch 13 training complete\n",
            "Cost on training data: 6577.874706955712\n",
            "Accuracy on training data: 48256 / 50000\n",
            "Cost on evaluation data: 6577.9106139534915\n",
            "Accuracy on evaluation data: 9585 / 10000\n",
            "\n",
            "Epoch 14 training complete\n",
            "Cost on training data: 6544.018377078277\n",
            "Accuracy on training data: 48070 / 50000\n",
            "Cost on evaluation data: 6544.0631842430685\n",
            "Accuracy on evaluation data: 9532 / 10000\n",
            "\n",
            "Epoch 15 training complete\n",
            "Cost on training data: 6544.189552796008\n",
            "Accuracy on training data: 48205 / 50000\n",
            "Cost on evaluation data: 6544.225850797902\n",
            "Accuracy on evaluation data: 9579 / 10000\n",
            "\n",
            "Epoch 16 training complete\n",
            "Cost on training data: 6578.625893190787\n",
            "Accuracy on training data: 48353 / 50000\n",
            "Cost on evaluation data: 6578.659235827081\n",
            "Accuracy on evaluation data: 9587 / 10000\n",
            "\n",
            "Epoch 17 training complete\n",
            "Cost on training data: 6610.037171090069\n",
            "Accuracy on training data: 48386 / 50000\n",
            "Cost on evaluation data: 6610.085102768767\n",
            "Accuracy on evaluation data: 9581 / 10000\n",
            "\n",
            "Epoch 18 training complete\n",
            "Cost on training data: 6589.37788936597\n",
            "Accuracy on training data: 48205 / 50000\n",
            "Cost on evaluation data: 6589.421361460353\n",
            "Accuracy on evaluation data: 9552 / 10000\n",
            "\n",
            "Epoch 19 training complete\n",
            "Cost on training data: 6586.184043163959\n",
            "Accuracy on training data: 48395 / 50000\n",
            "Cost on evaluation data: 6586.224469818697\n",
            "Accuracy on evaluation data: 9595 / 10000\n",
            "\n",
            "Epoch 20 training complete\n",
            "Cost on training data: 6627.815339842163\n",
            "Accuracy on training data: 48026 / 50000\n",
            "Cost on evaluation data: 6627.851696510694\n",
            "Accuracy on evaluation data: 9538 / 10000\n",
            "\n",
            "Epoch 21 training complete\n",
            "Cost on training data: 6622.793223737513\n",
            "Accuracy on training data: 48367 / 50000\n",
            "Cost on evaluation data: 6622.840663779933\n",
            "Accuracy on evaluation data: 9594 / 10000\n",
            "\n",
            "Epoch 22 training complete\n",
            "Cost on training data: 6652.464782271792\n",
            "Accuracy on training data: 48287 / 50000\n",
            "Cost on evaluation data: 6652.5089677698315\n",
            "Accuracy on evaluation data: 9567 / 10000\n",
            "\n",
            "Epoch 23 training complete\n",
            "Cost on training data: 6686.230562414743\n",
            "Accuracy on training data: 47883 / 50000\n",
            "Cost on evaluation data: 6686.2719191265805\n",
            "Accuracy on evaluation data: 9514 / 10000\n",
            "\n",
            "Epoch 24 training complete\n",
            "Cost on training data: 6651.412248003408\n",
            "Accuracy on training data: 48413 / 50000\n",
            "Cost on evaluation data: 6651.449805066104\n",
            "Accuracy on evaluation data: 9592 / 10000\n",
            "\n",
            "Epoch 25 training complete\n",
            "Cost on training data: 6676.0546925940835\n",
            "Accuracy on training data: 48414 / 50000\n",
            "Cost on evaluation data: 6676.100133315765\n",
            "Accuracy on evaluation data: 9610 / 10000\n",
            "\n",
            "Epoch 26 training complete\n",
            "Cost on training data: 6633.777527935005\n",
            "Accuracy on training data: 48223 / 50000\n",
            "Cost on evaluation data: 6633.814520238496\n",
            "Accuracy on evaluation data: 9579 / 10000\n",
            "\n",
            "Epoch 27 training complete\n",
            "Cost on training data: 6676.823663390585\n",
            "Accuracy on training data: 48342 / 50000\n",
            "Cost on evaluation data: 6676.866176928236\n",
            "Accuracy on evaluation data: 9585 / 10000\n",
            "\n",
            "Epoch 28 training complete\n",
            "Cost on training data: 6713.391570265182\n",
            "Accuracy on training data: 48370 / 50000\n",
            "Cost on evaluation data: 6713.43537070187\n",
            "Accuracy on evaluation data: 9607 / 10000\n",
            "\n",
            "Epoch 29 training complete\n",
            "Cost on training data: 6675.93181163052\n",
            "Accuracy on training data: 48380 / 50000\n",
            "Cost on evaluation data: 6675.976746790384\n",
            "Accuracy on evaluation data: 9608 / 10000\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([3873.626380047432,\n",
              "  4903.909219502273,\n",
              "  5470.3780924437515,\n",
              "  5749.556195050321,\n",
              "  6006.274159741172,\n",
              "  6127.0597776886025,\n",
              "  6234.38211034149,\n",
              "  6304.652229312478,\n",
              "  6354.4519082848,\n",
              "  6384.110704739801,\n",
              "  6454.511532804825,\n",
              "  6475.362266120982,\n",
              "  6539.0050191049995,\n",
              "  6577.9106139534915,\n",
              "  6544.0631842430685,\n",
              "  6544.225850797902,\n",
              "  6578.659235827081,\n",
              "  6610.085102768767,\n",
              "  6589.421361460353,\n",
              "  6586.224469818697,\n",
              "  6627.851696510694,\n",
              "  6622.840663779933,\n",
              "  6652.5089677698315,\n",
              "  6686.2719191265805,\n",
              "  6651.449805066104,\n",
              "  6676.100133315765,\n",
              "  6633.814520238496,\n",
              "  6676.866176928236,\n",
              "  6713.43537070187,\n",
              "  6675.976746790384],\n",
              " [9407,\n",
              "  9488,\n",
              "  9507,\n",
              "  9571,\n",
              "  9511,\n",
              "  9563,\n",
              "  9570,\n",
              "  9536,\n",
              "  9503,\n",
              "  9556,\n",
              "  9563,\n",
              "  9556,\n",
              "  9559,\n",
              "  9585,\n",
              "  9532,\n",
              "  9579,\n",
              "  9587,\n",
              "  9581,\n",
              "  9552,\n",
              "  9595,\n",
              "  9538,\n",
              "  9594,\n",
              "  9567,\n",
              "  9514,\n",
              "  9592,\n",
              "  9610,\n",
              "  9579,\n",
              "  9585,\n",
              "  9607,\n",
              "  9608],\n",
              " [3873.6261422439547,\n",
              "  4903.894639603323,\n",
              "  5470.352651205806,\n",
              "  5749.527335650388,\n",
              "  6006.232150178873,\n",
              "  6127.03040749656,\n",
              "  6234.335571196147,\n",
              "  6304.60879505393,\n",
              "  6354.417743889218,\n",
              "  6384.07506466922,\n",
              "  6454.477543697938,\n",
              "  6475.327314437201,\n",
              "  6538.964339594958,\n",
              "  6577.874706955712,\n",
              "  6544.018377078277,\n",
              "  6544.189552796008,\n",
              "  6578.625893190787,\n",
              "  6610.037171090069,\n",
              "  6589.37788936597,\n",
              "  6586.184043163959,\n",
              "  6627.815339842163,\n",
              "  6622.793223737513,\n",
              "  6652.464782271792,\n",
              "  6686.230562414743,\n",
              "  6651.412248003408,\n",
              "  6676.0546925940835,\n",
              "  6633.777527935005,\n",
              "  6676.823663390585,\n",
              "  6713.391570265182,\n",
              "  6675.93181163052],\n",
              " [47021,\n",
              "  47467,\n",
              "  47731,\n",
              "  48089,\n",
              "  47925,\n",
              "  48070,\n",
              "  48138,\n",
              "  48039,\n",
              "  47809,\n",
              "  48150,\n",
              "  48202,\n",
              "  48104,\n",
              "  48187,\n",
              "  48256,\n",
              "  48070,\n",
              "  48205,\n",
              "  48353,\n",
              "  48386,\n",
              "  48205,\n",
              "  48395,\n",
              "  48026,\n",
              "  48367,\n",
              "  48287,\n",
              "  47883,\n",
              "  48413,\n",
              "  48414,\n",
              "  48223,\n",
              "  48342,\n",
              "  48370,\n",
              "  48380])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "netQuadratic = Network2([784, 30, 10], cost=QuadraticCost)\n"
      ],
      "metadata": {
        "id": "JlPtN5Wa-fed"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "netQuadratic.SGD(training_data, 30, 10, 0.5,lmbda = 5.0,evaluation_data=validation_data,monitor_evaluation_accuracy=True,\n",
        "        monitor_evaluation_cost=True,monitor_training_accuracy=True,monitor_training_cost=True)"
      ],
      "metadata": {
        "id": "hIfi3hC8OkON",
        "outputId": "14e1813b-d8e5-47a7-a5d3-82a172323779",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 training complete\n",
            "Cost on training data: 1002.1075175958556\n",
            "Accuracy on training data: 45923 / 50000\n",
            "Cost on evaluation data: 1002.100117375537\n",
            "Accuracy on evaluation data: 9272 / 10000\n",
            "\n",
            "Epoch 1 training complete\n",
            "Cost on training data: 1206.2034131838761\n",
            "Accuracy on training data: 46515 / 50000\n",
            "Cost on evaluation data: 1206.1991286824898\n",
            "Accuracy on evaluation data: 9342 / 10000\n",
            "\n",
            "Epoch 2 training complete\n",
            "Cost on training data: 1305.504611364588\n",
            "Accuracy on training data: 46811 / 50000\n",
            "Cost on evaluation data: 1305.5007401291932\n",
            "Accuracy on evaluation data: 9425 / 10000\n",
            "\n",
            "Epoch 3 training complete\n",
            "Cost on training data: 1362.4190618319324\n",
            "Accuracy on training data: 47062 / 50000\n",
            "Cost on evaluation data: 1362.4162900418821\n",
            "Accuracy on evaluation data: 9444 / 10000\n",
            "\n",
            "Epoch 4 training complete\n",
            "Cost on training data: 1397.6613744401204\n",
            "Accuracy on training data: 47343 / 50000\n",
            "Cost on evaluation data: 1397.659212245384\n",
            "Accuracy on evaluation data: 9502 / 10000\n",
            "\n",
            "Epoch 5 training complete\n",
            "Cost on training data: 1420.7090287364636\n",
            "Accuracy on training data: 47298 / 50000\n",
            "Cost on evaluation data: 1420.7069564462038\n",
            "Accuracy on evaluation data: 9481 / 10000\n",
            "\n",
            "Epoch 6 training complete\n",
            "Cost on training data: 1438.6253821388268\n",
            "Accuracy on training data: 47374 / 50000\n",
            "Cost on evaluation data: 1438.623789171487\n",
            "Accuracy on evaluation data: 9493 / 10000\n",
            "\n",
            "Epoch 7 training complete\n",
            "Cost on training data: 1452.2190870154052\n",
            "Accuracy on training data: 47537 / 50000\n",
            "Cost on evaluation data: 1452.2172928382877\n",
            "Accuracy on evaluation data: 9543 / 10000\n",
            "\n",
            "Epoch 8 training complete\n",
            "Cost on training data: 1464.5105611625197\n",
            "Accuracy on training data: 47540 / 50000\n",
            "Cost on evaluation data: 1464.5089558690931\n",
            "Accuracy on evaluation data: 9543 / 10000\n",
            "\n",
            "Epoch 9 training complete\n",
            "Cost on training data: 1472.2352138508786\n",
            "Accuracy on training data: 47557 / 50000\n",
            "Cost on evaluation data: 1472.2337060918421\n",
            "Accuracy on evaluation data: 9535 / 10000\n",
            "\n",
            "Epoch 10 training complete\n",
            "Cost on training data: 1478.660545186937\n",
            "Accuracy on training data: 47625 / 50000\n",
            "Cost on evaluation data: 1478.6593928483185\n",
            "Accuracy on evaluation data: 9549 / 10000\n",
            "\n",
            "Epoch 11 training complete\n",
            "Cost on training data: 1484.1513733901895\n",
            "Accuracy on training data: 47660 / 50000\n",
            "Cost on evaluation data: 1484.1499551816016\n",
            "Accuracy on evaluation data: 9558 / 10000\n",
            "\n",
            "Epoch 12 training complete\n",
            "Cost on training data: 1491.372734511107\n",
            "Accuracy on training data: 47636 / 50000\n",
            "Cost on evaluation data: 1491.3713772977\n",
            "Accuracy on evaluation data: 9549 / 10000\n",
            "\n",
            "Epoch 13 training complete\n",
            "Cost on training data: 1493.4342072590205\n",
            "Accuracy on training data: 47654 / 50000\n",
            "Cost on evaluation data: 1493.4330571397456\n",
            "Accuracy on evaluation data: 9535 / 10000\n",
            "\n",
            "Epoch 14 training complete\n",
            "Cost on training data: 1494.572114786737\n",
            "Accuracy on training data: 47605 / 50000\n",
            "Cost on evaluation data: 1494.5710051198657\n",
            "Accuracy on evaluation data: 9538 / 10000\n",
            "\n",
            "Epoch 15 training complete\n",
            "Cost on training data: 1499.7925988369768\n",
            "Accuracy on training data: 47743 / 50000\n",
            "Cost on evaluation data: 1499.7910292690244\n",
            "Accuracy on evaluation data: 9569 / 10000\n",
            "\n",
            "Epoch 16 training complete\n",
            "Cost on training data: 1504.2808420326223\n",
            "Accuracy on training data: 47767 / 50000\n",
            "Cost on evaluation data: 1504.2794376803854\n",
            "Accuracy on evaluation data: 9566 / 10000\n",
            "\n",
            "Epoch 17 training complete\n",
            "Cost on training data: 1501.2893393264326\n",
            "Accuracy on training data: 47673 / 50000\n",
            "Cost on evaluation data: 1501.2882856788513\n",
            "Accuracy on evaluation data: 9551 / 10000\n",
            "\n",
            "Epoch 18 training complete\n",
            "Cost on training data: 1506.5807220421866\n",
            "Accuracy on training data: 47667 / 50000\n",
            "Cost on evaluation data: 1506.579383045939\n",
            "Accuracy on evaluation data: 9539 / 10000\n",
            "\n",
            "Epoch 19 training complete\n",
            "Cost on training data: 1506.619763565112\n",
            "Accuracy on training data: 47721 / 50000\n",
            "Cost on evaluation data: 1506.6184786916942\n",
            "Accuracy on evaluation data: 9575 / 10000\n",
            "\n",
            "Epoch 20 training complete\n",
            "Cost on training data: 1508.5569387640082\n",
            "Accuracy on training data: 47815 / 50000\n",
            "Cost on evaluation data: 1508.5553160138165\n",
            "Accuracy on evaluation data: 9575 / 10000\n",
            "\n",
            "Epoch 21 training complete\n",
            "Cost on training data: 1511.2659518742478\n",
            "Accuracy on training data: 47770 / 50000\n",
            "Cost on evaluation data: 1511.264808909499\n",
            "Accuracy on evaluation data: 9568 / 10000\n",
            "\n",
            "Epoch 22 training complete\n",
            "Cost on training data: 1515.6380993185694\n",
            "Accuracy on training data: 47791 / 50000\n",
            "Cost on evaluation data: 1515.6369382485084\n",
            "Accuracy on evaluation data: 9554 / 10000\n",
            "\n",
            "Epoch 23 training complete\n",
            "Cost on training data: 1516.0418060735587\n",
            "Accuracy on training data: 47737 / 50000\n",
            "Cost on evaluation data: 1516.0406406108805\n",
            "Accuracy on evaluation data: 9565 / 10000\n",
            "\n",
            "Epoch 24 training complete\n",
            "Cost on training data: 1519.0406268990685\n",
            "Accuracy on training data: 47741 / 50000\n",
            "Cost on evaluation data: 1519.039545272014\n",
            "Accuracy on evaluation data: 9555 / 10000\n",
            "\n",
            "Epoch 25 training complete\n",
            "Cost on training data: 1521.1161606699286\n",
            "Accuracy on training data: 47821 / 50000\n",
            "Cost on evaluation data: 1521.1144959155442\n",
            "Accuracy on evaluation data: 9579 / 10000\n",
            "\n",
            "Epoch 26 training complete\n",
            "Cost on training data: 1518.9302792629603\n",
            "Accuracy on training data: 47838 / 50000\n",
            "Cost on evaluation data: 1518.9294012982807\n",
            "Accuracy on evaluation data: 9589 / 10000\n",
            "\n",
            "Epoch 27 training complete\n",
            "Cost on training data: 1521.35063872595\n",
            "Accuracy on training data: 47761 / 50000\n",
            "Cost on evaluation data: 1521.349167021964\n",
            "Accuracy on evaluation data: 9562 / 10000\n",
            "\n",
            "Epoch 28 training complete\n",
            "Cost on training data: 1520.9775879346262\n",
            "Accuracy on training data: 47791 / 50000\n",
            "Cost on evaluation data: 1520.976468871597\n",
            "Accuracy on evaluation data: 9573 / 10000\n",
            "\n",
            "Epoch 29 training complete\n",
            "Cost on training data: 1521.1737504734583\n",
            "Accuracy on training data: 47803 / 50000\n",
            "Cost on evaluation data: 1521.1727615869238\n",
            "Accuracy on evaluation data: 9561 / 10000\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([1002.100117375537,\n",
              "  1206.1991286824898,\n",
              "  1305.5007401291932,\n",
              "  1362.4162900418821,\n",
              "  1397.659212245384,\n",
              "  1420.7069564462038,\n",
              "  1438.623789171487,\n",
              "  1452.2172928382877,\n",
              "  1464.5089558690931,\n",
              "  1472.2337060918421,\n",
              "  1478.6593928483185,\n",
              "  1484.1499551816016,\n",
              "  1491.3713772977,\n",
              "  1493.4330571397456,\n",
              "  1494.5710051198657,\n",
              "  1499.7910292690244,\n",
              "  1504.2794376803854,\n",
              "  1501.2882856788513,\n",
              "  1506.579383045939,\n",
              "  1506.6184786916942,\n",
              "  1508.5553160138165,\n",
              "  1511.264808909499,\n",
              "  1515.6369382485084,\n",
              "  1516.0406406108805,\n",
              "  1519.039545272014,\n",
              "  1521.1144959155442,\n",
              "  1518.9294012982807,\n",
              "  1521.349167021964,\n",
              "  1520.976468871597,\n",
              "  1521.1727615869238],\n",
              " [9272,\n",
              "  9342,\n",
              "  9425,\n",
              "  9444,\n",
              "  9502,\n",
              "  9481,\n",
              "  9493,\n",
              "  9543,\n",
              "  9543,\n",
              "  9535,\n",
              "  9549,\n",
              "  9558,\n",
              "  9549,\n",
              "  9535,\n",
              "  9538,\n",
              "  9569,\n",
              "  9566,\n",
              "  9551,\n",
              "  9539,\n",
              "  9575,\n",
              "  9575,\n",
              "  9568,\n",
              "  9554,\n",
              "  9565,\n",
              "  9555,\n",
              "  9579,\n",
              "  9589,\n",
              "  9562,\n",
              "  9573,\n",
              "  9561],\n",
              " [1002.1075175958556,\n",
              "  1206.2034131838761,\n",
              "  1305.504611364588,\n",
              "  1362.4190618319324,\n",
              "  1397.6613744401204,\n",
              "  1420.7090287364636,\n",
              "  1438.6253821388268,\n",
              "  1452.2190870154052,\n",
              "  1464.5105611625197,\n",
              "  1472.2352138508786,\n",
              "  1478.660545186937,\n",
              "  1484.1513733901895,\n",
              "  1491.372734511107,\n",
              "  1493.4342072590205,\n",
              "  1494.572114786737,\n",
              "  1499.7925988369768,\n",
              "  1504.2808420326223,\n",
              "  1501.2893393264326,\n",
              "  1506.5807220421866,\n",
              "  1506.619763565112,\n",
              "  1508.5569387640082,\n",
              "  1511.2659518742478,\n",
              "  1515.6380993185694,\n",
              "  1516.0418060735587,\n",
              "  1519.0406268990685,\n",
              "  1521.1161606699286,\n",
              "  1518.9302792629603,\n",
              "  1521.35063872595,\n",
              "  1520.9775879346262,\n",
              "  1521.1737504734583],\n",
              " [45923,\n",
              "  46515,\n",
              "  46811,\n",
              "  47062,\n",
              "  47343,\n",
              "  47298,\n",
              "  47374,\n",
              "  47537,\n",
              "  47540,\n",
              "  47557,\n",
              "  47625,\n",
              "  47660,\n",
              "  47636,\n",
              "  47654,\n",
              "  47605,\n",
              "  47743,\n",
              "  47767,\n",
              "  47673,\n",
              "  47667,\n",
              "  47721,\n",
              "  47815,\n",
              "  47770,\n",
              "  47791,\n",
              "  47737,\n",
              "  47741,\n",
              "  47821,\n",
              "  47838,\n",
              "  47761,\n",
              "  47791,\n",
              "  47803])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "name": "MNielsen_network.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}